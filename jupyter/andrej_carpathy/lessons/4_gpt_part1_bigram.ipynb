{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0424a9b4-cfac-47ab-a627-953a4cbd393f",
   "metadata": {},
   "source": [
    "# GPT - Part 1: Biogram\n",
    "- Video: [Andrej Karpathy - Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1413s)\n",
    "- Papers\n",
    "    - [Attention is All You Need paper](https://arxiv.org/abs/1706.03762)\n",
    "    - [OpenAI GPT-3 paper](https://arxiv.org/abs/2005.14165) \n",
    "    - [OpenAI ChatGPT blog post](https://openai.com/blog/chatgpt/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd5caaa-8781-47c8-80b8-50e5c6fcf58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# matpolitlib config\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29fdb04-67b3-4a9c-8db2-b9736443cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1450934f-754b-4083-9952-ef082a798d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all the words\n",
    "with open('./source/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4cc421-0a5e-4707-965d-ad8d9f39399b",
   "metadata": {},
   "source": [
    "## Simple encoder\n",
    "\n",
    "Common example of more sofisticated encoders are:\n",
    "- Google uses [SentencePiece](https://github.com/google/sentencepiece). SentencePiece implements subword units\n",
    "  (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]).\n",
    "- OpenAi uses [tiktoken](https://github.com/openai/tiktoken). tiktoken is a fast BPE tokeniser for use with\n",
    "  OpenAI's models. Example code using tiktoken can be found in the [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6292f004-5a6d-4ff0-8237-778e9f40ed05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0359d5-132a-4568-ba07-8eba095529fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 46, 47, 57, 1, 46, 53, 59, 57, 43]\n",
      "This house\n"
     ]
    }
   ],
   "source": [
    "print(encode('This house'))\n",
    "print(decode(encode('This house')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25b860-e82f-40a7-b6c1-b3bd7042e667",
   "metadata": {},
   "source": [
    "### Bonus example using tiktokenizer (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eac838d-6d63-4d8d-93b3-1962e2f7bb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "[464, 2068, 7586, 21831]\n",
      "The quick brown fox\n",
      "100277\n",
      "[791, 4062, 14198, 39935]\n",
      "The quick brown fox\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(enc.n_vocab)\n",
    "print(enc.encode(\"The quick brown fox\"))\n",
    "print(enc.decode(enc.encode(\"The quick brown fox\")))\n",
    "\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "print(enc.n_vocab)\n",
    "print(enc.encode(\"The quick brown fox\"))\n",
    "print(enc.decode(enc.encode(\"The quick brown fox\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506e4f9f-068e-428f-b79f-2f784686d0fe",
   "metadata": {},
   "source": [
    "## Set the initial values\n",
    "#### encode the text as tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b79d8de-c6f6-4dcd-ab77-7620263faa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.LongTensor\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.type())\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d56a98-b14b-41a3-970d-012a8dbe8254",
   "metadata": {},
   "source": [
    "#### split the data to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000626ee-a0cd-4502-a153-0640bf2508fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[n:]\n",
    "val_data = data[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b248d62-db34-4840-813f-77e0d035d66a",
   "metadata": {},
   "source": [
    "#### **block_size** is the fixed lenght blocks of data\n",
    "- what is the minimum context length for predictions?\n",
    "- how many characters do we look back on to predict the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb8c9091-2e8c-4d27-9857-6dd1779f3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ad337-0908-49cd-a1bc-689ae6af623c",
   "metadata": {},
   "source": [
    "#### **batch_size** is the number of sequential block sizes to run in parallel\n",
    "- how many independant sequences will we process in parallel\n",
    "- how many forward and backward passes in the training. Torch sorts the parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d70addf-7183-4f6f-950e-cab83ca901c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4fa12-89e7-475c-9c7b-bb6f0f8d3e13",
   "metadata": {},
   "source": [
    "#### set the global random seed for selecting batches from the text\n",
    "This is only to repeat same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b3c2124-a4bb-4b37-a77f-28fcf41be9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc617344690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f8baa-d470-4cc0-bcfd-8eeae861444c",
   "metadata": {},
   "source": [
    "## Get the batch split\n",
    "all the batch from 1 to batch_size (x) and all their targets (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c8be4c8-7dda-4bf5-923f-9958e6bb8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i: i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "# get a sigle train block\n",
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a0824c-3f8d-4373-9c35-178c87d0ba28",
   "metadata": {},
   "source": [
    "#### what we have for each block\n",
    "- block size of 4 by 8\n",
    "- note the target is the previous contect together as above\n",
    "- [1, 47] together in this order has a target 57\n",
    "- likewise [1, 47, 57] has a target 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f8f6ed5-a235-4488-8472-95a8a7ce466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 6,  1, 52, 53, 58,  1, 58, 47],\n",
      "        [ 6,  1, 54, 50, 39, 52, 58, 43],\n",
      "        [ 1, 58, 46, 47, 57,  1, 50, 47],\n",
      "        [ 0, 32, 46, 43, 56, 43,  1, 42]])\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 52, 53, 58,  1, 58, 47, 50],\n",
      "        [ 1, 54, 50, 39, 52, 58, 43, 58],\n",
      "        [58, 46, 47, 57,  1, 50, 47, 60],\n",
      "        [32, 46, 43, 56, 43,  1, 42, 53]])\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('inputs')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c3d72-4bf2-4901-b359-d4c0c21dd0a0",
   "metadata": {},
   "source": [
    "#### what we have for one batch block\n",
    "- batch size of 8 (time) the block size of 4 \n",
    "- this is an indepentant batch over time for 4 repeating blocks so we don't do this for all text when training\n",
    "- note the target represents the context tokens in this order and this size only\n",
    "- 6 followed by 1 followed by 52 has a probability that the next token will be 53\n",
    "- the target comes in at the end (ouput layer) to indicate the loss (how far were we from getting it right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1861ab1-521f-4e5c-a19b-fcb25fbe6886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [6]    the target is 1\n",
      "when input is [6, 1]    the target is 52\n",
      "when input is [6, 1, 52]    the target is 53\n",
      "when input is [6, 1, 52, 53]    the target is 58\n",
      "when input is [6, 1, 52, 53, 58]    the target is 1\n",
      "when input is [6, 1, 52, 53, 58, 1]    the target is 58\n",
      "when input is [6, 1, 52, 53, 58, 1, 58]    the target is 47\n",
      "when input is [6, 1, 52, 53, 58, 1, 58, 47]    the target is 50\n",
      "when input is [6]    the target is 1\n",
      "when input is [6, 1]    the target is 54\n",
      "when input is [6, 1, 54]    the target is 50\n",
      "when input is [6, 1, 54, 50]    the target is 39\n",
      "when input is [6, 1, 54, 50, 39]    the target is 52\n",
      "when input is [6, 1, 54, 50, 39, 52]    the target is 58\n",
      "when input is [6, 1, 54, 50, 39, 52, 58]    the target is 43\n",
      "when input is [6, 1, 54, 50, 39, 52, 58, 43]    the target is 58\n",
      "when input is [1]    the target is 58\n",
      "when input is [1, 58]    the target is 46\n",
      "when input is [1, 58, 46]    the target is 47\n",
      "when input is [1, 58, 46, 47]    the target is 57\n",
      "when input is [1, 58, 46, 47, 57]    the target is 1\n",
      "when input is [1, 58, 46, 47, 57, 1]    the target is 50\n",
      "when input is [1, 58, 46, 47, 57, 1, 50]    the target is 47\n",
      "when input is [1, 58, 46, 47, 57, 1, 50, 47]    the target is 60\n",
      "when input is [0]    the target is 32\n",
      "when input is [0, 32]    the target is 46\n",
      "when input is [0, 32, 46]    the target is 43\n",
      "when input is [0, 32, 46, 43]    the target is 56\n",
      "when input is [0, 32, 46, 43, 56]    the target is 43\n",
      "when input is [0, 32, 46, 43, 56, 43]    the target is 1\n",
      "when input is [0, 32, 46, 43, 56, 43, 1]    the target is 42\n",
      "when input is [0, 32, 46, 43, 56, 43, 1, 42]    the target is 53\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch timention\n",
    "    for t in range(block_size): # time dimention\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()}    the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5dad2-fb29-4c4a-a250-eb9a71487008",
   "metadata": {},
   "source": [
    "## Build a simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62214798-fcdf-4fb8-b93d-26aaa8affb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.7895, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "class BiogramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)  # (Batch Time Container)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # torch cross_entropt expects B C T if three dimentional so make it 2 dimentions\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            # targets to match logits\n",
    "            targets = targets.view(B*T)\n",
    "            # cross_entropy of output (logits) against target labels\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is the (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # use softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # returns (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # returns (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # returns (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BiogramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48acf07-11be-4fa1-be1b-d1d545f46da1",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "#### create the optimizer\n",
    "- Before we used Stocastic Gradient Decent, which is a very simple optimizer.\n",
    "- Now we are going to use Adam\n",
    "    - considered the best optimizer\n",
    "    - learning rate best at 3e-4\n",
    "    - simpler networks we can get away with higher learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e00526f-e6e3-4119-b747-30e1cb8298e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(m.parameters(), lr=1e-3)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e595e3d-b457-492d-965d-4e14dea96ada",
   "metadata": {},
   "source": [
    "#### typical training loop\n",
    "- reset the batch_size to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0190e87-f1f9-4079-8181-0cb7d48bb05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6662\n",
      "3.6984\n",
      "3.0699\n",
      "2.7992\n",
      "2.5873\n",
      "2.5084\n",
      "2.3623\n",
      "2.3209\n",
      "2.4716\n",
      "2.3864\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # track stats\n",
    "    if steps % 1000 == 0: \n",
    "        print(f'{loss.item():.4f}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967283b1-34ca-4738-950b-efd67c7e8602",
   "metadata": {},
   "source": [
    "#### optimizer results\n",
    "- Better looking results than 1 run\n",
    "- Not great because using the simples type of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "749a7930-2a9e-44c5-bd49-e195bc697b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ong h hasbe pave pirance\n",
      "GRO:\n",
      "Bagathathar's we!\n",
      "PeKAd ith henoangincenonthioneir thoniteay heltieiengerofo'PTIsit ey\n",
      "KANld pe wisher ve pllouthers nowl t,\n",
      "Kay ththind tt hinio t ouchos tes; sw yo hind\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=200)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8381d0-e10c-4d72-9671-7c851c00f372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0cb00-3743-499e-a10e-0e1f9f5dc863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b43d79-71eb-44f6-88bb-e29b131564b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572320ae-9237-4995-a256-33bf84eca113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
