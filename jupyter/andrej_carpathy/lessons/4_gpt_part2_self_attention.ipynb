{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0424a9b4-cfac-47ab-a627-953a4cbd393f",
   "metadata": {},
   "source": [
    "# GPT - Part 2: self-attention\n",
    "- Video: [Andrej Karpathy - Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1413s)\n",
    "- Paper\n",
    "    - [Attention is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3fa02-c252-4686-9620-34f58547688c",
   "metadata": {},
   "source": [
    "### What are transformers?\n",
    "Transformers are neural networks that specialize in learning context from the data. Quite similar to us trying to find the meaning of ‘attention and context’ in terms of transformers.\n",
    "\n",
    "### How do transformers learn context from the data?\n",
    "By using the attention mechanism.\n",
    "\n",
    "### What is the attention mechanism?\n",
    "The attention mechanism helps the model scan all parts of a sequence at each step and determine which elements need to be focused on. The attention mechanism was proposed as an alternative to the ‘strict/hard’ solution of fixed-length vectors in the encoder-decoder architecture and provide a ‘soft’ solution focusing only on the relevant parts.\n",
    "\n",
    "### What is self-attention?\n",
    "The attention mechanism worked to improve the performance of Recurrence Neural Networks (RNNs), with the effect seeping into Convolutional Neural Networks (CNNs). However, with the introduction of the transformer architecture in the year 2017, the need for RNNs and CNNs was quietly obliterated. And the central reason for it was the self-attention mechanism.\n",
    "\n",
    "The self-attention mechanism was special in the sense that it was built to inculcate the context of the input sequence in order to enhance the attention mechanism. This idea became transformational as it was able to capture the complex nuances of a language.\n",
    "\n",
    "There are many variations of how self-attention is performed. But the scaled dot-product mechanism has been one of the most popular ones. This was the one introduced in the original transformer architecture paper in 2017 — “Attention is All You Need”.\n",
    "\n",
    "### Where and how does self-attention feature in transformers?\n",
    "I like to see the transformer architecture as a combination of two shells — the outer shell and the inner shell.\n",
    "\n",
    "- The outer shell is a combination of the attention-weighting mechanism and the feed forward layer.\n",
    "- The inner shell consists of the self-attention mechanism and is part of the attention-weighting feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd5caaa-8781-47c8-80b8-50e5c6fcf58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9487f6-8da2-45df-9990-8fb53f31f088",
   "metadata": {},
   "source": [
    "## matematical trick to replace for loops\n",
    "#### imagine the toy example\n",
    "- Each batch has eight tokens pointing to 2 channels (4,8,2)\n",
    "- each token channel is a representative of itself\n",
    "- B   ==> Mini-batch: \n",
    "- T   ==> Time or Tokens: Attention scores relative to other words (tokens) in the sequence\n",
    "- C   ==> Channel: a feature map of learned patterns or features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead0cb00-3743-499e-a10e-0e1f9f5dc863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aecfc44-9207-4387-9d46-fe8c575d4065",
   "metadata": {},
   "source": [
    "## Batch time awareness\n",
    "### Python for loops\n",
    "- We want to discover the relationship with tokens in the past and from it predict tokens in the future.\n",
    "- The past tokens represent a single batch line\n",
    "- $ x[b,t] = \\bar{b}_{i<=t} [b,i] $\n",
    "- bow => bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572320ae-9237-4995-a256-33bf84eca113",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros(B,T,C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev,dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e3070b-12bc-438b-8e7a-170c6104a1e7",
   "metadata": {},
   "source": [
    "- unlike before each consecutive time is the mean of all previous times in that batch\n",
    "- but this code is slow and messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14dbd12-8db0-48a3-b6fc-a6655cc69138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00cdac5-9c17-44fc-99f4-d5e60fadd22c",
   "metadata": {},
   "source": [
    "### Weighted multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d320294-289e-4b45-918d-ec7aeee88fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b93ccc2-e878-4149-8021-dcaef6978783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.zeros((T,T))\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c386306-5630-4e7d-ab55-3c87ea9750a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a9248-7ae8-40fc-bd1c-c65d3fd698c4",
   "metadata": {},
   "source": [
    "#### softmax normalises the zeros to add to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d288bc3-eadb-43c1-913a-6932c4af64f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1)\n",
    "wei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568d3c9-fbba-4e60-88e0-10c4d138b16f",
   "metadata": {},
   "source": [
    "#### aggrigation through matrix multiplication\n",
    "dot product of wei(ght) with (B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23483027-34b7-48bb-927a-92e809985345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow1 = wei @ x\n",
    "xbow1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c74c6-c203-4409-97d9-f0e28f5bb5fc",
   "metadata": {},
   "source": [
    "Self attention is where the zeros are replaced by the token weights of the past elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4a1e3-88d6-4c87-883e-dc3383898253",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "self-attention for a single individual head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0d37289-0353-4d43-896d-0597403080fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # Batch, Time, Channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "#lets see a single Head perform self attention\n",
    "head_size = 16\n",
    "# this is what I know\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "# this is what I want\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "# fill with data\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "# create the wei(ght) for the head\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# no longer initialize to zero\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa8299-b121-483d-8450-e70f47b8d31f",
   "metadata": {},
   "source": [
    "#### this now tells us in a data dependant manner how much information to aggregate from the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0adaec80-a478-466d-b482-2b1c54fb5d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22c8b5f-b33e-4b98-a017-f8654f44d611",
   "metadata": {},
   "source": [
    "## Self-attention with single head\n",
    "### Random B T C setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a3f5edf-d9ef-4b37-98a5-0c257153ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # Batch, Time, Channels\n",
    "x = torch.randn(B,T,C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f6bab-9230-401f-83ac-0fc8e752a78a",
   "metadata": {},
   "source": [
    "### Attention weight matrix (A)\n",
    "The attention weight matrix A is obtained by feeding the input features into the Query-Key (QK) module. \n",
    "This matrix tries to find the most relevant parts in the input sequence. Self-Attention comes into play \n",
    "while creating the Attention weight matrix A using the QK-module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7434a082-38ae-46cd-85c4-e8c0aff28e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see a single Head perform self attention\n",
    "head_size = 16\n",
    "# multiply the features with linear transformation\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "# v is like a view of x for the purposes of these tokens\n",
    "v = value(x) # (B, T, 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641065d-3c9e-4789-8a2f-ab4b3132cc3f",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "- As can be seen from the calculation above, we use the same set of features for both queries and keys. And that is how the idea of “self” comes into play here, i.e. the model uses the same set of features to create its query vector as well as the key vector.\n",
    "- The query vector represents the current word (or token) for which we want to compute attention scores relative to other words in the sequence.\n",
    "- The key vector represents the other words (or tokens) in the input sequence and we compute the attention score for each of them with respect to the current word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6be188-1d96-402e-aaca-b278b3c9c3f4",
   "metadata": {},
   "source": [
    "### Multiply the transpose of K with Q\n",
    "The idea here is to calculate the dot product between every pair of query and key vectors. Calculating the dot \n",
    "product gives us an estimate of the matching score between every “key-query” pair, by using the idea of Cosine \n",
    "Similarity between the two vectors. This is the ‘dot-product’ part of the scaled dot-product attention.\n",
    "- ensure K transposes both the Token and the Head\n",
    "\n",
    "#### Cosine-Similarity\n",
    "Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided \n",
    "by the product of their lengths. It roughly measures if two vectors are pointing in the same direction thus implying the \n",
    "two vectors are similar.\n",
    "\n",
    "$$ \\mathnormal{Cos}(0^o) = 1, \\mathnormal{Cos}(90^o) = 0, \\mathnormal{Cos}(180^o) = -1  $$\n",
    "\n",
    "- If the dot product between the two vectors is approximately 1, it implies we are looking at an almost zero angle\n",
    "between the two vectors meaning they are very close to each other.\n",
    "- If the dot product between the two vectors is approximately 0, it implies we are looking at vectors that are\n",
    "orthogonal to each other and not very similar.\n",
    "- If the dot product between the two vectors is approximately -1, it implies we are looking at an almost an 180°\n",
    "angle between the two vectors meaning they are opposites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96da36ec-9351-4a34-ab1d-53f5d23522bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "weight = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b310102-047b-4ee2-8df1-493c29d7db25",
   "metadata": {},
   "source": [
    "### Attention Weight Matrix (A)\n",
    "- as in our math trick we mask, scale then Softmax\n",
    "- each column becomes a probability distribution of attention, which gives us our Attention Weight Matrix (A).\n",
    "\n",
    "#### Softmatrix\n",
    "- The Softmax step is important as it assigns probabilities to the score obtained in the previous steps and\n",
    "thus helps the model decide how much importance (higher/lower attention weights) needs to be given to each\n",
    "word given the current query. As is to be expected, higher attention weights signify greater relevance\n",
    "allowing the model to capture dependencies more accurately.\n",
    "- The scaling becomes important here. Without the scaling, the values of the resultant matrix gets pushed out\n",
    "into regions that are not processed well by the Softmax function and may result in vanishing gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b29e631d-7635-40a6-a7a1-afbe3126f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "weight = weight.masked_fill(tril == 0, float('-inf'))\n",
    "weight = F.softmax(weight, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed0fe5-0d2f-469d-9068-899e845e1350",
   "metadata": {},
   "source": [
    "### Attention weighted features\n",
    "Finally we multiply the value vectors (Vs) with the Attention Weight Matrix (A). These value vectors are important \n",
    "as they contain the information associated with each token in the sequence.\n",
    "\n",
    "attention weighted features are the ultimate solution of the self-attention mechanism. These attention-weighted \n",
    "features essentially contain a weighted representation of the features assigning higher weights for features with \n",
    "higher relevance as per the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc81ecf6-1357-4985-b731-651d013cfafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight is now calculated with v\n",
    "out = weight @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a960bd-b1b7-4a09-a2ca-0b83e7ddeb5b",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "Now with this information available, we continue to the next step in the transformer architecture where the \n",
    "feed-forward layer processes this information further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66162524-5bab-4aca-9b53-6768a38bbae3",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb4f7b-8d6d-4afa-9e59-ac945304aa6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
